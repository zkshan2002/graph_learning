## 1. 上次实验的问题
上次报告实验了HAN对不同比例的两种noise的影响，以及[SFT]提出的fluctuate filter和confidence penalty loss两个trick的表现。  
但由于参数设置不合理，得到的以下结论并不正确：

1. 在DBLP数据集上，提高noise ratio，f1下降不明显
2. [SFT]的filter几乎没有作用，因为fluctuate的node占比只有不到5%

主要有问题的超参是sample_limit，其含义是确定target_node后，从全部metapath instance中sample出的用于训练的metapath的数目上限。  
sample_limit增大，模型发现feature co-occurrence的能力增强，就会对noisy label有一定鲁棒性。  
之前的实验在DBLP上的sample_limit设置为512，远大于通常值100，于是得到结果中两个f1下降不明显。

| Uniform_Noise |    Macro-F1     |    Micro-F1     |    Accuracy     |
|:-------------:|:---------------:|:---------------:|:---------------:|
|      0.0      | 0.9393 ~ 0.0043 | 0.9430 ~ 0.0041 | 0.9382 ~ 0.0039 |
|      0.1      | 0.9362 ~ 0.0035 | 0.9403 ~ 0.0034 | 0.9331 ~ 0.0069 |
|      0.2      | 0.9351 ~ 0.0056 | 0.9393 ~ 0.0052 | 0.9188 ~ 0.0139 |
|      0.3      | 0.9361 ~ 0.0076 | 0.9403 ~ 0.0072 | 0.9153 ~ 0.0075 |
|      0.4      | 0.9165 ~ 0.0506 | 0.9217 ~ 0.0481 | 0.8747 ~ 0.0891 |
|      0.5      | 0.8873 ~ 0.0529 | 0.8937 ~ 0.0516 | 0.7942 ~ 0.0894 |

将sample_limit下调到128，就能得到这个结果：

| Uniform_Noise |    Macro-F1     |    Micro-F1     |    Accuracy     |
|:-------------:|:---------------:|:---------------:|:---------------:|
|      0.0      | 0.9303 ~ 0.0034 | 0.9349 ~ 0.0031 | 0.9351 ~ 0.0042 | 
|      0.1      | 0.9306 ~ 0.0049 | 0.9349 ~ 0.0046 | 0.9220 ~ 0.0079 | 
|      0.2      | 0.9275 ~ 0.0037 | 0.9324 ~ 0.0034 | 0.9167 ~ 0.0096 | 
|      0.3      | 0.9158 ~ 0.0050 | 0.9213 ~ 0.0042 | 0.8762 ~ 0.0181 | 
|      0.4      | 0.8782 ~ 0.0650 | 0.8842 ~ 0.0641 | 0.8286 ~ 0.0873 | 
|      0.5      | 0.7733 ~ 0.0643 | 0.7828 ~ 0.0627 | 0.6585 ~ 0.0731 | 

再下调到64，和128结果相近：

|               Tag                | Macro-F1 | Micro-F1 | Accuracy |
|:--------------------------------:|:--------:|:--------:|:--------:|
|       exp0 DBLP sample 64        |  0.9296  |  0.9342  |  0.9347  | 
| exp0 DBLP sample 64 noise_u 0.1  |  0.9251  |  0.9296  |  0.9223  | 
| exp0 DBLP sample 64 noise_u 0.2  |  0.9223  |  0.9275  |  0.9066  | 
| exp0 DBLP sample 64 noise_u 0.3  |  0.9161  |  0.9216  |  0.8888  | 
| exp0 DBLP sample 64 noise_u 0.4  |  0.8495  |  0.8590  |  0.7824  | 
| exp0 DBLP sample 64 noise_u 0.5  |  0.7811  |  0.7913  |  0.6606  |

似乎总的来说f1下降还是不很明显，例如noise=0.3时f1依然有0.91，这可能是因为DBLP数据集的节点分类任务简单。  
语义上，DBLP是一个作者论文网络，涉及作者A，论文P，术语T，会议C，metapath scheme有APA, APTPA, APCPA, 任务是将作者A分类到4个研究领域。
而作者A的feature是A的文章的key words的bag of words表示，从这个表示推断出研究方向似乎并不难。  
另一个数据集IMDB上的随noise_ratio上升，f1下降更明显，一方面因为数据集规模小，sample_limit一直设置为128，另一方面因为任务更难。

## 2. [SFT]的filtering

重新实验了[SFT]的filtering，为每个noise_ratio调表现最好的sft超参，结果如下：

sample_limit=128时

| Uniform_Noise | SFT_Filter |    Macro-F1     |    Micro-F1     |    Accuracy     |
|:-------------:|:----------:|:---------------:|:---------------:|:---------------:|
|      0.0      |     N      |     0.9303      |     0.9349      |     0.9351      |
|      0.1      |     N      |     0.9306      |     0.9349      |     0.9220      | 
|      0.1      |     Y      | 0.9301(-0.0005) | 0.9344(-0.0005) | 0.9221(+0.0001) |
|      0.2      |     N      |     0.9275      |     0.9324      |     0.9167      |
|      0.2      |     Y      | 0.9269(-0.0006) | 0.9317(-0.0007) | 0.9121(-0.0046) |
|      0.3      |     N      |     0.9158      |     0.9213      |     0.8762      |
|      0.3      |     Y      | 0.9239(+0.0081) | 0.9287(+0.0074) | 0.8910(+0.0148) |
|      0.4      |     N      |     0.8782      |     0.8842      |     0.8286      |
|      0.4      |     Y      | 0.8826(+0.0044) | 0.8887(+0.0045) | 0.8296(+0.0010) | 
|      0.5      |     N      |     0.7733      |     0.7828      |     0.6585      |
|      0.5      |     Y      | 0.7851(+0.0118) | 0.7946(+0.0118) | 0.6681(+0.0096) | 

总体而言无显著作用。  
noise_ratio很小时，无显著变化，noise_ratio很大时至多提升1%。  
通过实验，波动点占比不稳定，大约占到0%到10%.

sample_limit=64时

| Uniform_Noise | SFT_Filter |    Macro-F1     |    Micro-F1     |    Accuracy     |
|:-------------:|:----------:|:---------------:|:---------------:|:---------------:|
|      0.0      |     N      |     0.9296      |     0.9342      |     0.9347      |
|      0.1      |     N      |     0.9251      |     0.9296      |     0.9223      | 
|      0.1      |     Y      | 0.9273(+0.0022) | 0.9320(+0.0024) | 0.9240(+0.0017) |
|      0.2      |     N      |     0.9223      |     0.9275      |     0.9066      |
|      0.2      |     Y      | 0.9224(+0.0001) | 0.9277(+0.0002) | 0.8957(-0.0109) |
|      0.3      |     N      |     0.9161      |     0.9216      |     0.8888      |
|      0.3      |     Y      | 0.9204(+0.0043) | 0.9256(+0.0040) | 0.9024(+0.0136) |
|      0.4      |     N      |     0.8495      |     0.8590      |     0.7824      |
|      0.4      |     Y      | 0.9013(+0.0518) | 0.9075(+0.0485) | 0.8356(+0.0532) | 
|      0.5      |     N      |     0.7811      |     0.7913      |     0.6389      |
|      0.5      |     Y      | 0.7981(+0.0160) | 0.8083(+0.0170) | 0.6794(+0.0405) | 

在noise_ratio较大时有效果，0.4处提升5%（很奇怪），0.5处提升1.5%。  
模型本身在noise_ratio较大时表现很不稳定，例如noise_ratio=0.5时的5次实验的macro_f1为
0.834775 | 0.813695 | 0.690252 | 0.738810 | 0.827888，得到0.781084 ~ 0.056876  
模型在noise很严重时的不稳定性，或许可以一定程度上解释加入sft_filter后表现的不稳定。  
波动点占比依然不到10%。

## 3. [MLC]的meta learning correction

sample_limit=64, T_lr=0.2时

| Uniform_Noise | MLC |    Macro-F1     |    Micro-F1     |    Accuracy     |
|:-------------:|:---:|:---------------:|:---------------:|:---------------:|
|      0.0      |  N  |     0.9296      |     0.9342      |     0.9347      |
|      0.1      |  N  |     0.9251      |     0.9296      |     0.9223      | 
|      0.1      |  Y  | 0.9243(-0.0008) | 0.9293(-0.0003) | 0.9232(+0.0009) |
|      0.2      |  N  |     0.9223      |     0.9275      |     0.9066      |
|      0.2      |  Y  | 0.9264(+0.0041) | 0.9314(+0.0039) | 0.9154(+0.0088) |
|      0.3      |  N  |     0.9161      |     0.9216      |     0.8888      |
|      0.3      |  Y  | 0.9177(+0.0016) | 0.9230(+0.0014) | 0.8943(+0.0055) |
|      0.4      |  N  |     0.8495      |     0.8590      |     0.7824      |
|      0.4      |  Y  | 0.8769(+0.0274) | 0.8840(+0.0250) | 0.8190(+0.0366) | 
|      0.5      |  N  |     0.7811      |     0.7913      |     0.6389      |
|      0.5      |  Y  | 0.7936(+0.0125) | 0.8029(+0.0116) | 0.6636(+0.0247) | 

在noise_ratio较大时有效果，依然是0.4处提升大于0.5.
学到的T并不符合实际，而接近uniform noise = 0.01的情况，即每列(0.99, 0.003, 0.003, 0.003)。
增大T的学习率，学到的T更加接近真实值，但整体表现并没有提升：

|                  Tag                   | Macro-F1 | Micro-F1 | Accuracy |
|:--------------------------------------:|:--------:|:--------:|:--------:|
| exp4 DBLP64 noise_u 0.4 mlc 0.005 0.05 |  0.8655  |  0.8723  |  0.7920  | 
| exp4 DBLP64 noise_u 0.4 mlc 0.005 0.10 |  0.8746  |  0.8815  |  0.8020  | 
| exp4 DBLP64 noise_u 0.4 mlc 0.005 0.20 |  0.8769  |  0.8840  |  0.8190  | 
| exp4 DBLP64 noise_u 0.4 mlc 0.005 0.5  |  0.8630  |  0.8698  |  0.7876  | 
|  exp4 DBLP64 noise_u 0.4 mlc 0.005 1   |  0.8502  |  0.8576  |  0.7666  | 
|  exp4 DBLP64 noise_u 0.4 mlc 0.005 2   |  0.8547  |  0.8619  |  0.7700  | 
|  exp4 DBLP64 noise_u 0.4 mlc 0.005 5   |  0.8608  |  0.8678  |  0.7609  | 
|  exp4 DBLP64 noise_u 0.4 mlc 0.005 10  |  0.8266  |  0.8358  |  0.6990  | 

下面观察实验得到的T.其格式为：T_ij表示预测的label i应为label j的概率，即j被flip到i的概率。  
noise_u = 0.4时T的真实值如下：

|     |   0    |   1    |   2    |   3    |
|:---:|:------:|:------:|:------:|:------:|
|  0  | 0.6000 | 0.1333 | 0.1333 | 0.1333 |
|  1  | 0.1333 | 0.6000 | 0.1333 | 0.1333 |
|  2  | 0.1333 | 0.1333 | 0.6000 | 0.1333 |
|  3  | 0.1333 | 0.1333 | 0.1333 | 0.6000 |

学习率为5时，几次实验得到的T如下：

|     |   0    |   1    |   2    |   3    |
|:---:|:------:|:------:|:------:|:------:|
|  0  | 1.0000 | 0.1486 | 0.0102 | 0.0403 |
|  1  | 0.0000 | 0.7380 | 0.0000 | 0.0000 |
|  2  | 0.0000 | 0.0671 | 0.9586 | 0.0048 |
|  3  | 0.0000 | 0.0462 | 0.0312 | 0.9549 |

|     |   0    |   1    |   2    |   3    |
|:---:|:------:|:------:|:------:|:------:|
|  0  | 0.9565 | 0.1163 | 0.0402 | 0.0175 |
|  1  | 0.0179 | 0.7105 | 0.0014 | 0.0041 |
|  2  | 0.0241 | 0.1090 | 0.8836 | 0.0558 |
|  3  | 0.0015 | 0.0641 | 0.0748 | 0.9226 |

|     |   0    |   1    |   2    |   3    |
|:---:|:------:|:------:|:------:|:------:|
|  0  | 0.9426 | 0.1119 | 0.0429 | 0.0488 |
|  1  | 0.0377 | 0.8474 | 0.0138 | 0.0137 |
|  2  | 0.0198 | 0.0247 | 0.9293 | 0.0352 |
|  3  | 0.0000 | 0.0161 | 0.0141 | 0.9024 |

学习率为5已经足够高，但模型学到的T依然不够充分，接近noise_u=0.1时的真实值。  
其他学习率的结果相似，T接近真实值对应的noise_u提高，但始终不到0.1.最高表现在0.2处。

结论：目前实验的MLC得到的noise transition matrix和真实值有差异，不过在noise_ratio较大时也有一定效果，能提高1～2%的f1.

在另一个数据集IMDB上，两个方法都没有显著提升，对f1的影响在0.5%以内。

|                  Tag                   | Macro-F1 | Micro-F1 | Accuracy |
|:--------------------------------------:|:--------:|:--------:|:--------:|
|        exp6 IMDB64 noise_u 0.1         |  0.6011  |  0.6031  |  0.5823  | 
| exp6 IMDB64 noise_u 0.1 sft_filter 1 2 |  0.6023  |  0.6042  |  0.5818  | 
| exp6 IMDB64 noise_u 0.1 sft_filter 1 4 |  0.6025  |  0.6043  |  0.5800  | 
| exp6 IMDB64 noise_u 0.1 sft_filter 1 6 |  0.6016  |  0.6038  |  0.5843  | 
| exp6 IMDB64 noise_u 0.1 sft_filter 1 8 |  0.6032  |  0.6050  |  0.5812  | 
|        exp6 IMDB64 noise_u 0.2         |  0.5811  |  0.5836  |  0.5486  | 
| exp6 IMDB64 noise_u 0.2 sft_filter 1 2 |  0.5805  |  0.5830  |  0.5501  | 
| exp6 IMDB64 noise_u 0.2 sft_filter 1 4 |  0.5828  |  0.5852  |  0.5508  | 
| exp6 IMDB64 noise_u 0.2 sft_filter 1 6 |  0.5821  |  0.5846  |  0.5501  | 
| exp6 IMDB64 noise_u 0.2 sft_filter 1 8 |  0.5825  |  0.5851  |  0.5492  | 
|        exp6 IMDB64 noise_u 0.3         |  0.5600  |  0.5634  |  0.5017  | 
| exp6 IMDB64 noise_u 0.3 sft_filter 1 2 |  0.5589  |  0.5623  |  0.5018  | 
| exp6 IMDB64 noise_u 0.3 sft_filter 1 4 |  0.5613  |  0.5648  |  0.5020  | 
| exp6 IMDB64 noise_u 0.3 sft_filter 1 6 |  0.5602  |  0.5636  |  0.5003  |
| exp6 IMDB64 noise_u 0.3 sft_filter 1 8 |  0.5598  |  0.5632  |  0.4984  | 
|        exp6 IMDB64 noise_u 0.4         |  0.5360  |  0.5394  |  0.4756  | 
| exp6 IMDB64 noise_u 0.4 sft_filter 1 2 |  0.5355  |  0.5389  |  0.4730  | 
| exp6 IMDB64 noise_u 0.4 sft_filter 1 4 |  0.5356  |  0.5390  |  0.4711  | 
| exp6 IMDB64 noise_u 0.4 sft_filter 1 6 |  0.5345  |  0.5383  |  0.4722  | 
| exp6 IMDB64 noise_u 0.4 sft_filter 1 8 |  0.5360  |  0.5396  |  0.4727  | 
|        exp6 IMDB64 noise_u 0.5         |  0.5110  |  0.5175  |  0.4271  | 
| exp6 IMDB64 noise_u 0.5 sft_filter 1 2 |  0.5101  |  0.5166  |  0.4235  | 
| exp6 IMDB64 noise_u 0.5 sft_filter 1 4 |  0.5095  |  0.5162  |  0.4252  | 
| exp6 IMDB64 noise_u 0.5 sft_filter 1 6 |  0.5098  |  0.5160  |  0.4262  | 
| exp6 IMDB64 noise_u 0.5 sft_filter 1 8 |  0.5102  |  0.5162  |  0.4265  | 

|               Tag                | Macro-F1 | Micro-F1 | Accuracy |
|:--------------------------------:|:--------:|:--------:|:--------:|
|     exp6 IMDB64 noise_u 0.1      |  0.6011  |  0.6031  |  0.5823  | 
| exp7 IMDB64 noise_u 0.1 mlc 0.05 |  0.6004  |  0.6026  |  0.5808  |
| exp7 IMDB64 noise_u 0.1 mlc 0.1  |  0.6010  |  0.6031  |  0.5808  | 
| exp7 IMDB64 noise_u 0.1 mlc 0.2  |  0.6007  |  0.6028  |  0.5805  | 
| exp7 IMDB64 noise_u 0.1 mlc 0.5  |  0.6005  |  0.6027  |  0.5815  | 
|  exp7 IMDB64 noise_u 0.1 mlc 1   |  0.6004  |  0.6025  |  0.5814  | 
|  exp7 IMDB64 noise_u 0.1 mlc 2   |  0.6009  |  0.6032  |  0.5824  | 
|  exp7 IMDB64 noise_u 0.1 mlc 5   |  0.5991  |  0.6011  |  0.5837  | 
|     exp6 IMDB64 noise_u 0.2      |  0.5811  |  0.5836  |  0.5486  | 
| exp7 IMDB64 noise_u 0.2 mlc 0.05 |  0.5790  |  0.5818  |  0.5506  |
| exp7 IMDB64 noise_u 0.2 mlc 0.1  |  0.5790  |  0.5818  |  0.5504  | 
| exp7 IMDB64 noise_u 0.2 mlc 0.2  |  0.5791  |  0.5819  |  0.5502  | 
| exp7 IMDB64 noise_u 0.2 mlc 0.5  |  0.5787  |  0.5815  |  0.5502  | 
|  exp7 IMDB64 noise_u 0.2 mlc 1   |  0.5789  |  0.5818  |  0.5504  | 
|  exp7 IMDB64 noise_u 0.2 mlc 2   |  0.5793  |  0.5822  |  0.5506  | 
|  exp7 IMDB64 noise_u 0.2 mlc 5   |  0.5797  |  0.5825  |  0.5507  | 
|     exp6 IMDB64 noise_u 0.3      |  0.5600  |  0.5634  |  0.5017  | 
| exp7 IMDB64 noise_u 0.3 mlc 0.05 |  0.5570  |  0.5608  |  0.5011  | 
| exp7 IMDB64 noise_u 0.3 mlc 0.1  |  0.5579  |  0.5617  |  0.5015  | 
| exp7 IMDB64 noise_u 0.3 mlc 0.2  |  0.5575  |  0.5612  |  0.5019  | 
| exp7 IMDB64 noise_u 0.3 mlc 0.5  |  0.5574  |  0.5612  |  0.5025  | 
|  exp7 IMDB64 noise_u 0.3 mlc 1   |  0.5572  |  0.5609  |  0.5041  | 
|  exp7 IMDB64 noise_u 0.3 mlc 2   |  0.5580  |  0.5611  |  0.5109  | 
|  exp7 IMDB64 noise_u 0.3 mlc 5   |  0.5579  |  0.5610  |  0.5121  |
|     exp6 IMDB64 noise_u 0.4      |  0.5360  |  0.5394  |  0.4756  | 
| exp7 IMDB64 noise_u 0.4 mlc 0.05 |  0.5313  |  0.5346  |  0.4779  | 
| exp7 IMDB64 noise_u 0.4 mlc 0.1  |  0.5311  |  0.5344  |  0.4782  | 
| exp7 IMDB64 noise_u 0.4 mlc 0.2  |  0.5303  |  0.5336  |  0.4782  | 
| exp7 IMDB64 noise_u 0.4 mlc 0.5  |  0.5303  |  0.5336  |  0.4780  | 
|  exp7 IMDB64 noise_u 0.4 mlc 1   |  0.5301  |  0.5331  |  0.4782  | 
|  exp7 IMDB64 noise_u 0.4 mlc 2   |  0.5296  |  0.5326  |  0.4791  | 
|  exp7 IMDB64 noise_u 0.4 mlc 5   |  0.5297  |  0.5328  |  0.4773  |
|     exp6 IMDB64 noise_u 0.5      |  0.5110  |  0.5175  |  0.4271  | 
| exp7 IMDB64 noise_u 0.5 mlc 0.05 |  0.5109  |  0.5177  |  0.4273  | 
| exp7 IMDB64 noise_u 0.5 mlc 0.1  |  0.5106  |  0.5173  |  0.4276  | 
| exp7 IMDB64 noise_u 0.5 mlc 0.2  |  0.5112  |  0.5180  |  0.4275  | 
| exp7 IMDB64 noise_u 0.5 mlc 0.5  |  0.5112  |  0.5178  |  0.4275  | 
|  exp7 IMDB64 noise_u 0.5 mlc 1   |  0.5089  |  0.5156  |  0.4266  | 
|  exp7 IMDB64 noise_u 0.5 mlc 2   |  0.5094  |  0.5165  |  0.4270  | 
|  exp7 IMDB64 noise_u 0.5 mlc 5   |  0.5096  |  0.5166  |  0.4260  | 

## 4. 可能的问题

1. 数据集稳定性
HAN在DBLP数据集上表现不太稳定，多个random seed实验的结果有差异，此差异随noise_ratio增大而显著增大。  
例如DBLP64, noise_u=0.4时，5次实验的macro_f1:  
0.783294 | 0.906418 | 0.786426 | 0.900826 | 0.870724  
差异高达10个点以上。  
而在另一个数据集IMDB上，差异很小。  
例如IMDB64, noise_u=0.4时，5次实验的macro_f1:  
0.531541 | 0.546651 | 0.526300 | 0.544209 | 0.531228  
差异在2个点以内。  
这可能可以解释为何SFT和MLC在DBLP上有一定的效果，但难以总结规律，同时在IMDB上没有显著效果。

2. 训练终止条件  
训练的early stop条件是连续5(DBLP)/10(IMDB)个epoch validation loss不下降，训练终止后选择validation loss最低的epoch.  
可能的问题是，validation loss由gnn和classification head给出，而head容易受noisy label影响
（增加noise_ratio, test accuracy下降比f1 score快）。  
从而终止条件可能不够可靠，造成模型表现评价不公平。

3. 训练数据比例不合理  
两个数据集带label的node都约为4000个，其中400个用于train，400个用于validate，3200个用于test.   
可能training node太少，导致模型训练不稳定。