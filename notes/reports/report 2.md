上次报告实验了HAN对不同比例的两种noise的影响，以及[SFT]提出的fluctuate filter和confidence penalty loss两个trick的表现
但由于参数设置不合理，得到的以下结论并不正确：

1. 在DBLP数据集上，提高noise ratio，f1下降不明显
2. [SFT]的filter几乎没有作用，因为fluctuate的node占比只有不到5%

主要有问题的超参是sample_limit，其含义是确定target_node后，从全部metapath instance中sample出的用于训练的metapath的数目上限。
sample_limit增大，模型发现feature co-occurrence的能力增强，就会对noisy label有一定鲁棒性。
之前的实验在DBLP上的sample_limit设置为512，远大于通常值100，于是得到结果中两个f1下降不明显。

| Uniform_Noise |    Macro-F1     |    Micro-F1     |    Accuracy     |
|:-------------:|:---------------:|:---------------:|:---------------:|
|      0.0      | 0.9393 ~ 0.0043 | 0.9430 ~ 0.0041 | 0.9382 ~ 0.0039 |
|      0.1      | 0.9362 ~ 0.0035 | 0.9403 ~ 0.0034 | 0.9331 ~ 0.0069 |
|      0.2      | 0.9351 ~ 0.0056 | 0.9393 ~ 0.0052 | 0.9188 ~ 0.0139 |
|      0.3      | 0.9361 ~ 0.0076 | 0.9403 ~ 0.0072 | 0.9153 ~ 0.0075 |
|      0.4      | 0.9165 ~ 0.0506 | 0.9217 ~ 0.0481 | 0.8747 ~ 0.0891 |
|      0.5      | 0.8873 ~ 0.0529 | 0.8937 ~ 0.0516 | 0.7942 ~ 0.0894 |

将sample_limit下调到128，就能得到这个结果：

| Uniform_Noise |    Macro-F1     |    Micro-F1     |    Accuracy     |
|:-------------:|:---------------:|:---------------:|:---------------:|
|      0.0      | 0.9303 ~ 0.0034 | 0.9349 ~ 0.0031 | 0.9351 ~ 0.0042 | 
|      0.1      | 0.9306 ~ 0.0049 | 0.9349 ~ 0.0046 | 0.9220 ~ 0.0079 | 
|      0.2      | 0.9275 ~ 0.0037 | 0.9324 ~ 0.0034 | 0.9167 ~ 0.0096 | 
|      0.3      | 0.9158 ~ 0.0050 | 0.9213 ~ 0.0042 | 0.8762 ~ 0.0181 | 
|      0.4      | 0.8782 ~ 0.0650 | 0.8842 ~ 0.0641 | 0.8286 ~ 0.0873 | 
|      0.5      | 0.7733 ~ 0.0643 | 0.7828 ~ 0.0627 | 0.6585 ~ 0.0731 | 

再下调到64，和128结果相近：

|               Tag                | Macro-F1 | Micro-F1 | Accuracy |
|:--------------------------------:|:--------:|:--------:|:--------:|
|       exp0 DBLP sample 64        |  0.9296  |  0.9342  |  0.9347  | 
| exp0 DBLP sample 64 noise_u 0.1  |  0.9251  |  0.9296  |  0.9223  | 
| exp0 DBLP sample 64 noise_u 0.2  |  0.9223  |  0.9275  |  0.9066  | 
| exp0 DBLP sample 64 noise_u 0.3  |  0.9161  |  0.9216  |  0.8888  | 
| exp0 DBLP sample 64 noise_u 0.4  |  0.8495  |  0.8590  |  0.7824  | 
| exp0 DBLP sample 64 noise_u 0.5  |  0.7811  |  0.7913  |  0.6606  |

似乎总的来说f1下降还是不很明显，例如noise=0.3时f1依然有0.91，这可能是因为DBLP数据集的节点分类任务简单。
语义上，DBLP是一个作者论文网络，涉及作者A，论文P，术语T，会议C，metapath scheme有APA, APTPA, APCPA, 任务是将作者A分类到4个研究领域。
而作者A的feature是A的文章的key words的bag of words表示，从这个表示推断出研究方向似乎并不难。
另一个数据集IMDB上的随noise_ratio上升，f1下降更明显，一方面因为数据集规模小，sample_limit一直设置为128，另一方面因为任务更难。

为了保证效果明显，随后的实验在sample_limit=64下进行。

重新实验了[SFT]的filtering，为每个noise_ratio调表现最好的sft超参，结果如下：

128

| Uniform_Noise | SFT_Filter |    Macro-F1     |    Micro-F1     |    Accuracy     |
|:-------------:|:----------:|:---------------:|:---------------:|:---------------:|
|      0.0      |     N      |     0.9303      |     0.9349      |     0.9351      |
|      0.1      |     N      |     0.9306      |     0.9349      |     0.9220      | 
|      0.1      |     Y      | 0.9301(-0.0005) |  0.9344(-0005)  | 0.9221(+0.0001) |
|      0.2      |     N      |     0.9275      |     0.9324      |     0.9167      |
|      0.2      |     Y      | 0.9269(-0.0006) | 0.9317(-0.0007) | 0.9121(-0.0046) |
|      0.3      |     N      |     0.9158      |     0.9213      |     0.8762      |
|      0.3      |     Y      | 0.9239(+0.0081) | 0.9287(+0.0074) | 0.8910(+0.0148) |
|      0.4      |     N      |     0.8782      |     0.8842      |     0.8286      |
|      0.4      |     Y      | 0.8826(+0.0044) | 0.8887(+0.0045) | 0.8296(+0.0010) | 
|      0.5      |     N      |     0.7733      |     0.7828      |     0.6585      |
|      0.5      |     Y      | 0.7851(+0.0118) | 0.7946(+0.0118) | 0.6681(+0.0096) | 

总体而言无显著作用。
noise_ratio很小时，无显著变化，noise_ratio很大时至多提升1%。
通过实验，波动点占比不稳定，大约占到0%到10%.

但在另一数据集IMDB上，依然没有明显效果（这是调超参的实验）：

|                 Tag                  | Macro-F1 | Micro-F1 | Accuracy |
|:------------------------------------:|:--------:|:--------:|:--------:|
|        exp0 IMDB noise_u 0.4         |  0.5359  |  0.5396  |  0.4770  | 
| exp1 IMDB noise_u 0.4 sft_filter 1 2 |  0.5355  |  0.5389  |  0.4730  | 
| exp1 IMDB noise_u 0.4 sft_filter 1 4 |  0.5356  |  0.5390  |  0.4711  | 
| exp1 IMDB noise_u 0.4 sft_filter 1 6 |  0.5345  |  0.5383  |  0.4722  | 
| exp1 IMDB noise_u 0.4 sft_filter 1 8 |  0.5360  |  0.5396  |  0.4727  | 
| exp1 IMDB noise_u 0.4 sft_filter 2 2 |  0.5355  |  0.5387  |  0.4777  | 
| exp1 IMDB noise_u 0.4 sft_filter 2 4 |  0.5341  |  0.5373  |  0.4713  | 
| exp1 IMDB noise_u 0.4 sft_filter 2 6 |  0.5359  |  0.5396  |  0.4632  | 
| exp1 IMDB noise_u 0.4 sft_filter 2 8 |  0.5339  |  0.5373  |  0.4657  | 

